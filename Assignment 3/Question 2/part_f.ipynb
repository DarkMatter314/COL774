{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier from scikit learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = './x_train.npy'\n",
    "y_train_path = './y_train.npy'\n",
    "\n",
    "X_train, y_train = get_data(x_train_path, y_train_path)\n",
    "\n",
    "x_test_path = './x_test.npy'\n",
    "y_test_path = './y_test.npy'\n",
    "\n",
    "X_test, y_test = get_data(x_test_path, y_test_path)\n",
    "\n",
    "#you might need one hot encoded y in part a,b,c,d,e\n",
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecures = [[512], [512, 256], [512, 256, 128], [512, 256, 128, 64]]\n",
    "classifiers = []\n",
    "opfile = open('part_f.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting architecture: [512]\n",
      "Iteration 1, loss = 0.97515181\n",
      "Iteration 2, loss = 0.83174944\n",
      "Iteration 3, loss = 0.76978568\n",
      "Iteration 4, loss = 0.76795605\n",
      "Iteration 5, loss = 0.76702198\n",
      "Iteration 6, loss = 0.76641146\n",
      "Iteration 7, loss = 0.76580071\n",
      "Iteration 8, loss = 0.76529058\n",
      "Iteration 9, loss = 0.76477996\n",
      "Iteration 10, loss = 0.76417849\n",
      "Iteration 11, loss = 0.76401693\n",
      "Iteration 12, loss = 0.76354859\n",
      "Iteration 13, loss = 0.76318742\n",
      "Iteration 14, loss = 0.76293184\n",
      "Iteration 15, loss = 0.76260204\n",
      "Iteration 16, loss = 0.76229978\n",
      "Iteration 17, loss = 0.76210018\n",
      "Iteration 18, loss = 0.76190156\n",
      "Iteration 19, loss = 0.76158466\n",
      "Iteration 20, loss = 0.76137316\n",
      "Iteration 21, loss = 0.76110987\n",
      "Iteration 22, loss = 0.76088191\n",
      "Iteration 23, loss = 0.76065378\n",
      "Iteration 24, loss = 0.76046978\n",
      "Iteration 25, loss = 0.76015724\n",
      "Iteration 26, loss = 0.76007899\n",
      "Iteration 27, loss = 0.76003575\n",
      "Iteration 28, loss = 0.75977805\n",
      "Iteration 29, loss = 0.75962438\n",
      "Iteration 30, loss = 0.75941563\n",
      "Iteration 31, loss = 0.75936116\n",
      "Iteration 32, loss = 0.75906301\n",
      "Iteration 33, loss = 0.75891189\n",
      "Iteration 34, loss = 0.75883389\n",
      "Iteration 35, loss = 0.75871031\n",
      "Iteration 36, loss = 0.75858672\n",
      "Iteration 37, loss = 0.75847786\n",
      "Iteration 38, loss = 0.75833626\n",
      "Iteration 39, loss = 0.75821134\n",
      "Iteration 40, loss = 0.75803962\n",
      "Iteration 41, loss = 0.75792208\n",
      "Iteration 42, loss = 0.75783683\n",
      "Iteration 43, loss = 0.75770261\n",
      "Iteration 44, loss = 0.75757641\n",
      "Iteration 45, loss = 0.75744148\n",
      "Iteration 46, loss = 0.75731921\n",
      "Iteration 47, loss = 0.75720744\n",
      "Iteration 48, loss = 0.75704505\n",
      "Iteration 49, loss = 0.75704855\n",
      "Iteration 50, loss = 0.75682618\n",
      "Iteration 51, loss = 0.75680725\n",
      "Iteration 52, loss = 0.75667280\n",
      "Iteration 53, loss = 0.75651703\n",
      "Iteration 54, loss = 0.75648984\n",
      "Iteration 55, loss = 0.75638812\n",
      "Iteration 56, loss = 0.75626805\n",
      "Iteration 57, loss = 0.75614285\n",
      "Iteration 58, loss = 0.75608129\n",
      "Iteration 59, loss = 0.75599693\n",
      "Iteration 60, loss = 0.75589636\n",
      "Iteration 61, loss = 0.75581050\n",
      "Iteration 62, loss = 0.75569342\n",
      "Iteration 63, loss = 0.75564450\n",
      "Iteration 64, loss = 0.75548839\n",
      "Iteration 65, loss = 0.75537510\n",
      "Iteration 66, loss = 0.75540385\n",
      "Iteration 67, loss = 0.75527044\n",
      "Iteration 68, loss = 0.75521829\n",
      "Iteration 69, loss = 0.75507713\n",
      "Iteration 70, loss = 0.75498565\n",
      "Iteration 71, loss = 0.75490494\n",
      "Iteration 72, loss = 0.75483458\n",
      "Iteration 73, loss = 0.75475880\n",
      "Iteration 74, loss = 0.75467424\n",
      "Iteration 75, loss = 0.75464504\n",
      "Iteration 76, loss = 0.75449918\n",
      "Iteration 77, loss = 0.75443444\n",
      "Iteration 78, loss = 0.75438629\n",
      "Iteration 79, loss = 0.75429503\n",
      "Iteration 80, loss = 0.75421196\n",
      "Iteration 81, loss = 0.75407981\n",
      "Iteration 82, loss = 0.75416413\n",
      "Iteration 83, loss = 0.75398891\n",
      "Iteration 84, loss = 0.75387852\n",
      "Iteration 85, loss = 0.75386511\n",
      "Iteration 86, loss = 0.75379677\n",
      "Iteration 87, loss = 0.75368420\n",
      "Iteration 88, loss = 0.75362443\n",
      "Iteration 89, loss = 0.75357342\n",
      "Iteration 90, loss = 0.75346670\n",
      "Iteration 91, loss = 0.75337912\n",
      "Iteration 92, loss = 0.75330177\n",
      "Iteration 93, loss = 0.75328902\n",
      "Iteration 94, loss = 0.75320549\n",
      "Iteration 95, loss = 0.75316525\n",
      "Iteration 96, loss = 0.75309831\n",
      "Iteration 97, loss = 0.75296936\n",
      "Iteration 98, loss = 0.75292273\n",
      "Iteration 99, loss = 0.75285344\n",
      "Iteration 100, loss = 0.75280827\n",
      "Iteration 101, loss = 0.75275001\n",
      "Iteration 102, loss = 0.75269280\n",
      "Iteration 103, loss = 0.75262048\n",
      "Iteration 104, loss = 0.75254881\n",
      "Iteration 105, loss = 0.75242401\n",
      "Iteration 106, loss = 0.75238904\n",
      "Iteration 107, loss = 0.75231854\n",
      "Iteration 108, loss = 0.75224621\n",
      "Iteration 109, loss = 0.75222913\n",
      "Iteration 110, loss = 0.75218661\n",
      "Iteration 111, loss = 0.75208637\n",
      "Iteration 112, loss = 0.75207765\n",
      "Iteration 113, loss = 0.75203487\n",
      "Iteration 114, loss = 0.75195594\n",
      "Iteration 115, loss = 0.75187861\n",
      "Iteration 116, loss = 0.75180348\n",
      "Iteration 117, loss = 0.75177303\n",
      "Iteration 118, loss = 0.75169414\n",
      "Iteration 119, loss = 0.75158529\n",
      "Iteration 120, loss = 0.75155595\n",
      "Iteration 121, loss = 0.75152085\n",
      "Iteration 122, loss = 0.75146172\n",
      "Iteration 123, loss = 0.75140667\n",
      "Iteration 124, loss = 0.75136560\n",
      "Iteration 125, loss = 0.75125490\n",
      "Iteration 126, loss = 0.75124619\n",
      "Iteration 127, loss = 0.75124835\n",
      "Iteration 128, loss = 0.75115930\n",
      "Iteration 129, loss = 0.75106441\n",
      "Iteration 130, loss = 0.75102903\n",
      "Iteration 131, loss = 0.75095361\n",
      "Iteration 132, loss = 0.75090859\n",
      "Iteration 133, loss = 0.75086812\n",
      "Iteration 134, loss = 0.75081356\n",
      "Iteration 135, loss = 0.75077867\n",
      "Iteration 136, loss = 0.75070618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Trained architecture: [512]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\IIT Delhi\\Sem 5\\1. COL774\\Assignment 3\\Part_B_data\\part_f.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_train_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results_train \u001b[39m=\u001b[39m get_metric(y_train_onehot, y_train_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m results_test \u001b[39m=\u001b[39m get_metric(y_test_onehot, y_test_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m opfile\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresults_train\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTesting Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresults_test\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\IIT Delhi\\Sem 5\\1. COL774\\Assignment 3\\Part_B_data\\part_f.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_metric\u001b[39m(y_true, y_pred):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m        y_true: np array of [NUM_SAMPLES x r] (one hot) \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m                \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     results \u001b[39m=\u001b[39m classification_report(y_pred, y_true)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/IIT%20Delhi/Sem%205/1.%20COL774/Assignment%203/Part_B_data/part_f.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:2539\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m   2406\u001b[0m     {\n\u001b[0;32m   2407\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2430\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2431\u001b[0m ):\n\u001b[0;32m   2432\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m \n\u001b[0;32m   2434\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2536\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2537\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2539\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   2541\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2542\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "for arch in architecures:\n",
    "    clf = MLPClassifier(solver='sgd', \n",
    "                        alpha=0, \n",
    "                        hidden_layer_sizes=arch, \n",
    "                        activation='relu', \n",
    "                        batch_size=32, \n",
    "                        learning_rate='invscaling', \n",
    "                        learning_rate_init=0.01,\n",
    "                        verbose=True)\n",
    "    print(f\"Starting architecture: {arch}\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Trained architecture: {arch}\")\n",
    "    classifiers.append(clf)\n",
    "    opfile.write(f\"Architecture: {arch}\\n\")\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    results_train = get_metric(y_train, y_train_pred)\n",
    "    results_test = get_metric(y_test, y_test_pred)\n",
    "    opfile.write(f\"Training Data:\\n{results_train}\\nTesting Data:\\n{results_test}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
