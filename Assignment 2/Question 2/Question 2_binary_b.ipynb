{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "from cvxopt import matrix, solvers\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_3_path = './svm/train/3/'\n",
    "image_3_list = []\n",
    "for filename in os.listdir(class_3_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(class_3_path,filename))\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (16, 16))\n",
    "            normalized_img = resized_img / 255.0\n",
    "            flattened_img = normalized_img.flatten()\n",
    "            image_3_list.append(flattened_img)\n",
    "\n",
    "class_4_path = './svm/train/4/'\n",
    "image_4_list = []\n",
    "for filename in os.listdir(class_4_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(class_4_path,filename))\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (16, 16))\n",
    "            normalized_img = resized_img / 255.0\n",
    "            flattened_img = normalized_img.flatten()\n",
    "            image_4_list.append(flattened_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4760, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(image_3_list) + len(image_4_list)\n",
    "n = image_4_list[0].shape[0]\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_3_list + image_4_list)\n",
    "Y = np.array([-1] * len(image_3_list) + [1] * len(image_4_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_matrix(X, gamma=0.001):\n",
    "    squared_distances = np.sum(X**2, axis=1, keepdims=True) - 2 * np.dot(X, X.T) + np.sum(X**2, axis=1, keepdims=True).T\n",
    "    kernel_matrix = np.exp(-gamma * squared_distances)\n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, gamma=0.001):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = gaussian_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4760)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.reshape(-1, 1).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = H * (Y[:, np.newaxis] * Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = matrix(PP, tc='d')\n",
    "q = matrix(-np.ones((m, 1)), tc='d')\n",
    "G = matrix(np.vstack((np.eye(m)*-1,np.eye(m))), tc='d')\n",
    "h = matrix(np.hstack((np.zeros(m), np.ones(m) * 1)), tc='d')\n",
    "A = matrix(Y.reshape(1, -1), tc='d')\n",
    "b = matrix(np.zeros(1), tc='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(10):\n",
    "    one = random.randint(0, 4000)\n",
    "    for j in range(10):\n",
    "       two = random.randint(0, 4000)\n",
    "       if(abs(H[one, two] - gaussian_kernel(X[one], X[two])) > 1e-15):\n",
    "           print(f\"Error at {one}, {two}, H = {H[one, two]}, K = {gaussian_kernel(X[one], X[two])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.4912e+03 -1.3863e+04  6e+04  3e+00  4e-13\n",
      " 1: -2.4344e+03 -1.0058e+04  1e+04  2e-01  4e-13\n",
      " 2: -2.6495e+03 -3.7859e+03  1e+03  2e-02  4e-13\n",
      " 3: -3.0479e+03 -3.3180e+03  3e+02  4e-03  4e-13\n",
      " 4: -3.1344e+03 -3.2401e+03  1e+02  1e-03  4e-13\n",
      " 5: -3.1712e+03 -3.2042e+03  3e+01  3e-04  4e-13\n",
      " 6: -3.1838e+03 -3.1918e+03  8e+00  6e-05  4e-13\n",
      " 7: -3.1874e+03 -3.1882e+03  8e-01  5e-06  5e-13\n",
      " 8: -3.1878e+03 -3.1878e+03  4e-02  2e-07  5e-13\n",
      " 9: -3.1878e+03 -3.1878e+03  2e-03  8e-09  5e-13\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "sol = solvers.qp(P, q, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array(sol['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3436"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sv = len(alphas[alphas > 1e-5])\n",
    "num_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors = []\n",
    "y_sv = []\n",
    "alpha_sv = []\n",
    "for i in range(m):\n",
    "    if(alphas[i] > 1e-5):\n",
    "        support_vectors.append(X[i])\n",
    "        alpha_sv.append(alphas[i])\n",
    "        y_sv.append(Y[i])\n",
    "support_vectors = np.array(support_vectors)\n",
    "alpha_sv = np.array(alpha_sv)\n",
    "y_sv = np.array(y_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.781297920126035"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_list = []\n",
    "# for i in range(m):\n",
    "#     if(alphas[i] < 1e-5 or (1 - alphas[i]) < 1e-5): continue\n",
    "kernel_matrix_bias = gaussian_matrix(support_vectors)\n",
    "new_matrix = kernel_matrix_bias * alpha_sv.reshape(-1, 1) * y_sv.reshape(-1, 1)\n",
    "bias_list = y_sv - np.sum(new_matrix, axis=0)\n",
    "b = 0\n",
    "num = 0\n",
    "for i in range(len(bias_list)):\n",
    "    if(alpha_sv[i] < 1e-5 or (1 - alpha_sv[i]) < 1e-5): continue\n",
    "    b += bias_list[i]\n",
    "    num += 1\n",
    "b /= num\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, support_vectors, b, gamma = 0.001):\n",
    "    prediction =  np.sum(np.exp(-0.001*np.sum((support_vectors - x)**2, axis = 1)) * y_sv * alpha_sv.reshape(alpha_sv.shape[0], ))  + b\n",
    "    return 1 if prediction >= 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_3_path = './svm/val/3/'\n",
    "validation_image_3_list = []\n",
    "for filename in os.listdir(validation_3_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(validation_3_path,filename))\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (16, 16))\n",
    "            normalized_img = resized_img / 255.0\n",
    "            flattened_img = normalized_img.flatten()\n",
    "            validation_image_3_list.append(flattened_img)\n",
    "validation_4_path = './svm/val/4/'\n",
    "validation_image_4_list = []\n",
    "for filename in os.listdir(validation_4_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img = cv2.imread(os.path.join(validation_4_path,filename))\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (16, 16))\n",
    "            normalized_img = resized_img / 255.0\n",
    "            flattened_img = normalized_img.flatten()\n",
    "            validation_image_4_list.append(flattened_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 311 \n",
      "Incorrect: 89 \n",
      "Accuracy: 0.7775\n",
      "Confusion Matrix: \n",
      "[[151, 49], [40, 160]]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "confusion_matrix = [[0, 0], [0, 0]]\n",
    "for img in validation_image_3_list:\n",
    "    prediction = predict(img, support_vectors, b)\n",
    "    if(prediction == -1):\n",
    "        correct += 1\n",
    "        confusion_matrix[0][0] += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        confusion_matrix[0][1] += 1\n",
    "for img in validation_image_4_list:\n",
    "    prediction = predict(img, support_vectors, b)\n",
    "    if(prediction == 1):\n",
    "        correct += 1\n",
    "        confusion_matrix[1][1] += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        confusion_matrix[1][0] += 1\n",
    "print(f\"Correct: {correct} \\nIncorrect: {incorrect} \\nAccuracy: {correct / (correct + incorrect)}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3490\n",
      "Incorrect: 1270\n",
      "Accuracy: 0.7331932773109243\n"
     ]
    }
   ],
   "source": [
    "train_incorrect = 0\n",
    "for img in image_3_list:\n",
    "    prediction = predict(img, support_vectors, b)\n",
    "    if(prediction != -1):\n",
    "        train_incorrect += 1\n",
    "for img in image_4_list:\n",
    "    prediction = predict(img, support_vectors, b)\n",
    "    if(prediction != 1):\n",
    "        train_incorrect += 1\n",
    "print(f\"Correct: {m - train_incorrect}\\nIncorrect: {train_incorrect}\\nAccuracy: {(m - train_incorrect) / m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(alphas, axis=0)[::-1]\n",
    "top_6 = sorted_indices[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_6_support_vectors = []\n",
    "image_3_names = os.listdir(class_3_path)\n",
    "image_4_names = os.listdir(class_4_path)\n",
    "image_3_sv = []\n",
    "image_4_sv = []\n",
    "for i in top_6:\n",
    "    if i[0] < len(image_3_names):\n",
    "        top_6_support_vectors.append(image_3_names[i[0]])\n",
    "        image_3_sv.append(image_3_names[i[0]])\n",
    "    else:\n",
    "        top_6_support_vectors.append(image_4_names[i[0] - len(image_3_names)])\n",
    "        image_4_sv.append(image_4_names[i[0] - len(image_3_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "output_dir = './part_b_sv/'\n",
    "i = 0\n",
    "for img_name in image_3_sv:\n",
    "    img = cv2.imread(os.path.join(class_3_path, img_name))\n",
    "    if img is not None:\n",
    "        resized_img = cv2.resize(img, (16, 16))\n",
    "        pillow_image_resized = Image.fromarray(np.uint8(resized_img))\n",
    "        pillow_image_original = Image.fromarray(np.uint8(img))\n",
    "        filename_resized = f\"sv_{i}_resized.jpg\"\n",
    "        filename_original = f\"sv_{i}_original.jpg\"\n",
    "        pillow_image_resized.save(os.path.join(output_dir, filename_resized))\n",
    "        pillow_image_original.save(os.path.join(output_dir, filename_original))\n",
    "    i += 1\n",
    "for img_name in image_4_sv:\n",
    "    img = cv2.imread(os.path.join(class_4_path, img_name))\n",
    "    if img is not None:\n",
    "        resized_img = cv2.resize(img, (16, 16))\n",
    "        pillow_image_resized = Image.fromarray(np.uint8(resized_img))\n",
    "        pillow_image_original = Image.fromarray(np.uint8(img))\n",
    "        filename_resized = f\"sv_{i}_resized.jpg\"\n",
    "        filename_original = f\"sv_{i}_original.jpg\"\n",
    "        pillow_image_resized.save(os.path.join(output_dir, filename_resized))\n",
    "        pillow_image_original.save(os.path.join(output_dir, filename_original))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1458, 1446])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_indices = clf.support_\n",
    "support_vectors = clf.support_vectors_\n",
    "my_indices = []\n",
    "for i in range(len(alphas)):\n",
    "    if(alphas[i] > 1e-5):\n",
    "        my_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_indices_set = set(my_indices)\n",
    "support_vector_indices_set = set(support_vector_indices)\n",
    "\n",
    "# Find the common elements (matching indices) using set intersection\n",
    "matching_indices = my_indices_set.intersection(support_vector_indices_set)\n",
    "\n",
    "# Get the count of matching indices\n",
    "num_matching_indices = len(matching_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2686"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_matching_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
