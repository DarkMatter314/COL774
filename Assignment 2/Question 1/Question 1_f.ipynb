{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./Corona_train.csv').to_numpy()\n",
    "domain_1_data = pd.read_csv('./Domain_Adaptation/Twitter_train_1.csv').to_numpy()\n",
    "domain_2_data = pd.read_csv('./Domain_Adaptation/Twitter_train_2.csv').to_numpy()\n",
    "domain_5_data = pd.read_csv('./Domain_Adaptation/Twitter_train_5.csv').to_numpy()\n",
    "domain_10_data = pd.read_csv('./Domain_Adaptation/Twitter_train_10.csv').to_numpy()\n",
    "domain_25_data = pd.read_csv('./Domain_Adaptation/Twitter_train_25.csv').to_numpy()\n",
    "domain_50_data = pd.read_csv('./Domain_Adaptation/Twitter_train_50.csv').to_numpy()\n",
    "domain_100_data = pd.read_csv('./Domain_Adaptation/Twitter_train_100.csv').to_numpy()\n",
    "domain_validation_data = pd.read_csv('./Domain_Adaptation/Twitter_validation.csv').to_numpy()\n",
    "stemmer = PorterStemmer()\n",
    "stopwords_set = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [domain_1_data, domain_2_data, domain_5_data, domain_10_data, domain_25_data, domain_50_data, domain_100_data]\n",
    "data_name = ['domain_1', 'domain_2', 'domain_5', 'domain_10', 'domain_25', 'domain_50', 'domain_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_punc_stopwords(data):\n",
    "    for i in range(len(data)):\n",
    "        text = data[i][2].lower()\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in punctuation]\n",
    "        changed_words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        data[i][2] = ' '.join(changed_words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(inputData):\n",
    "    allWords = []\n",
    "    dictWord = {}\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if (word != ' ') and (word not in dictWord):\n",
    "                dictWord[word] = len(allWords)\n",
    "                allWords.append(word)\n",
    "    return (allWords, dictWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(inputData, dictWord):\n",
    "    p_wc = np.zeros((3, len(dictWord)))\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if word not in dictWord: continue\n",
    "            if(data[1] == 'Positive'): p_wc[0][dictWord[word]] += 1\n",
    "            elif(data[1] == 'Neutral'): p_wc[1][dictWord[word]] += 1\n",
    "            else: p_wc[2][dictWord[word]] += 1\n",
    "    total = [0, 0, 0]\n",
    "    total[0] = sum(p_wc[0])\n",
    "    total[1] = sum(p_wc[1])\n",
    "    total[2] = sum(p_wc[2])\n",
    "    for i in range(len(total)):\n",
    "        for j in range(len(p_wc[i])):\n",
    "            p_wc[i][j] = (p_wc[i][j] + 1) / (total[i] + len(dictWord))\n",
    "    return p_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_punc_predict(text, dictWord, pc, p_wc):\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    words = [word for word in text if word not in punctuation]\n",
    "    p = np.zeros((3,))\n",
    "    for i in range(len(p)):\n",
    "        p[i] = np.log(pc[i])\n",
    "        for word in words:\n",
    "            if(word in stopwords_set): continue\n",
    "            word = stemmer.stem(word)\n",
    "            if(word in dictWord):\n",
    "                p[i] += np.log(p_wc[i][dictWord[word]])\n",
    "    return np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('q1_only_domain.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(i)\n",
    "    X = []\n",
    "    if(i==0): X = domain_1_data\n",
    "    if(i==1): X = domain_2_data\n",
    "    if(i==2): X = domain_5_data\n",
    "    if(i==3): X = domain_10_data\n",
    "    if(i==4): X = domain_25_data\n",
    "    if(i==5): X = domain_50_data\n",
    "    if(i==6): X = domain_100_data\n",
    "    X = stem_punc_stopwords(X)\n",
    "    (allWords, dictWord) = get_word_frequency(X)\n",
    "    file.write(f\"Domain: {data_name[i]}\\n\")\n",
    "    pc = [0, 0, 0]\n",
    "    pc[0] = X[X[:, 1] == 'Positive'].shape[0] / X.shape[0]\n",
    "    pc[1] = X[X[:, 1] == 'Neutral'].shape[0] / X.shape[0]\n",
    "    pc[2] = X[X[:, 1] == 'Negative'].shape[0] / X.shape[0]\n",
    "    p_wc = parameters(X, dictWord)\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for data in domain_validation_data:\n",
    "        prediction = stem_punc_predict(data[2], dictWord, pc, p_wc)\n",
    "        if(data[1] == 'Positive' and prediction == 0): correct += 1\n",
    "        elif(data[1] == 'Neutral' and prediction == 1): correct += 1\n",
    "        elif(data[1] == 'Negative' and prediction == 2): correct += 1\n",
    "        else: incorrect += 1\n",
    "    file.write(f\"Correct: {correct}\\nIncorrect: {incorrect}\\nAccuracy: {correct / (correct + incorrect)}\\n\")\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
