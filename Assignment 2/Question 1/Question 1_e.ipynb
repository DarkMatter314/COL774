{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./Corona_train.csv').to_numpy()\n",
    "transformed_data = pd.read_csv('./Corona_train.csv').to_numpy()\n",
    "validation_data = pd.read_csv('./Corona_validation.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords_set = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_punc_stopwords(data):\n",
    "    for i in range(len(data)):\n",
    "        text = data[i][2].lower()\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in punctuation]\n",
    "        changed_words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        data[i][2] = ' '.join(changed_words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = stem_punc_stopwords(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(inputData):\n",
    "    allWords = []\n",
    "    dictWord = {}\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if (word != ' ') and (word not in dictWord):\n",
    "                dictWord[word] = len(allWords)\n",
    "                allWords.append(word)\n",
    "    return (allWords, dictWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(inputData):\n",
    "    bigrams_list = []\n",
    "    bigrams_dict = {}\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for i in range(len(text) - 1):\n",
    "            bigram = text[i] + ' ' + text[i + 1]\n",
    "            if (bigram != ' ') and (bigram not in bigrams_dict):\n",
    "                bigrams_dict[bigram] = len(bigrams_list)\n",
    "                bigrams_list.append(bigram)\n",
    "    return (bigrams_list, bigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_words, dict_word) = get_word_frequency(transformed_data)\n",
    "(bigrams_list, dict_bigrams) = get_bigrams(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.zeros((3,))\n",
    "pc[0] = training_data[training_data[:, 1] == 'Positive'].shape[0] / training_data.shape[0]\n",
    "pc[1] = training_data[training_data[:, 1] == 'Neutral'].shape[0] / training_data.shape[0]\n",
    "pc[2] = training_data[training_data[:, 1] == 'Negative'].shape[0] / training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(inputData, dictWord, dict_bigrams):\n",
    "    p_wc = np.zeros((3, len(dictWord)))\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if word not in dictWord: continue\n",
    "            if(data[1] == 'Positive'): p_wc[0][dictWord[word]] += 1\n",
    "            elif(data[1] == 'Neutral'): p_wc[1][dictWord[word]] += 1\n",
    "            else: p_wc[2][dictWord[word]] += 1\n",
    "    total = [0, 0, 0]\n",
    "    total[0] = sum(p_wc[0])\n",
    "    total[1] = sum(p_wc[1])\n",
    "    total[2] = sum(p_wc[2])\n",
    "    for i in range(len(total)):\n",
    "        for j in range(len(p_wc[i])):\n",
    "            p_wc[i][j] = (p_wc[i][j] + 1) / (total[i] + len(dictWord))\n",
    "    p_wc_bigrams = np.zeros((3, len(dict_bigrams)))\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for i in range(len(text) - 1):\n",
    "            bigram = text[i] + ' ' + text[i + 1]\n",
    "            if bigram not in dict_bigrams: continue\n",
    "            if(data[1] == 'Positive'): p_wc_bigrams[0][dict_bigrams[bigram]] += 1\n",
    "            elif(data[1] == 'Neutral'): p_wc_bigrams[1][dict_bigrams[bigram]] += 1\n",
    "            else: p_wc_bigrams[2][dict_bigrams[bigram]] += 1\n",
    "    total_bigrams = [0, 0, 0]\n",
    "    total_bigrams[0] = sum(p_wc_bigrams[0])\n",
    "    total_bigrams[1] = sum(p_wc_bigrams[1])\n",
    "    total_bigrams[2] = sum(p_wc_bigrams[2])\n",
    "    for i in range(len(total_bigrams)):\n",
    "        for j in range(len(p_wc_bigrams[i])):\n",
    "            p_wc_bigrams[i][j] = (p_wc_bigrams[i][j] + 1) / (total_bigrams[i] + len(dict_bigrams))\n",
    "    return (p_wc, p_wc_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_wc_single, p_wc_bigrams) = parameters(transformed_data, dict_word, dict_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_predict(text, dict_single, dict_bigrams, p_wc_single, p_wc_bigrams):\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in text if ((word not in punctuation) and (word not in stopwords_set))]\n",
    "    p = np.zeros((3,))\n",
    "    for i in range(len(p)):\n",
    "        p[i] = np.log(pc[i])\n",
    "        for word in words:\n",
    "            if(word in dict_single):\n",
    "                p[i] += np.log(p_wc_single[i][dict_single[word]])\n",
    "        for j in range(len(words) - 1):\n",
    "            bigram = words[j] + ' ' + words[j + 1]\n",
    "            if(bigram in dict_bigrams):\n",
    "                p[i] += np.log(p_wc_bigrams[i][dict_bigrams[bigram]])\n",
    "    return np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct = 0\n",
    "train_incorrect = 0\n",
    "for data in training_data:\n",
    "    prediction = bigram_predict(data[2], dict_word, dict_bigrams, p_wc_single, p_wc_bigrams)\n",
    "    if(prediction == 0 and data[1] == 'Positive'): train_correct += 1\n",
    "    elif(prediction == 1 and data[1] == 'Neutral'): train_correct += 1\n",
    "    elif(prediction == 2 and data[1] == 'Negative'): train_correct += 1\n",
    "    else: train_incorrect += 1\n",
    "\n",
    "valid_correct = 0\n",
    "valid_incorrect = 0\n",
    "for data in validation_data:\n",
    "    prediction = bigram_predict(data[2], dict_word, dict_bigrams, p_wc_single, p_wc_bigrams)\n",
    "    if(prediction == 0 and data[1] == 'Positive'): valid_correct += 1\n",
    "    elif(prediction == 1 and data[1] == 'Neutral'): valid_correct += 1\n",
    "    elif(prediction == 2 and data[1] == 'Negative'): valid_correct += 1\n",
    "    else: valid_incorrect += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Correct: 36616\n",
      "Incorrect: 1248\n",
      "Accuracy: 0.967039932389605\n",
      "Validation\n",
      "Correct: 2275\n",
      "Incorrect: 1018\n",
      "Accuracy: 0.6908593987245673\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training\\nCorrect: {train_correct}\\nIncorrect: {train_incorrect}\\nAccuracy: {train_correct / (train_correct + train_incorrect)}\")\n",
    "print(f\"Validation\\nCorrect: {valid_correct}\\nIncorrect: {valid_incorrect}\\nAccuracy: {valid_correct / (valid_correct + valid_incorrect)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
