{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./Corona_train.csv').to_numpy()\n",
    "validation_data = pd.read_csv('./Corona_validation.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords_set = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.zeros((3,))\n",
    "pc[0] = training_data[training_data[:, 1] == 'Positive'].shape[0] / training_data.shape[0]\n",
    "pc[1] = training_data[training_data[:, 1] == 'Neutral'].shape[0] / training_data.shape[0]\n",
    "pc[2] = training_data[training_data[:, 1] == 'Negative'].shape[0] / training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_extra = np.zeros((3, 4))\n",
    "train_positive = training_data[training_data[:, 1] == 'Positive']\n",
    "train_neutral = training_data[training_data[:, 1] == 'Neutral']\n",
    "train_negative = training_data[training_data[:, 1] == 'Negative']\n",
    "# 1 - Exclamation Marks\n",
    "positive_exclam = np.sum(np.array([data[2].count('!') for data in train_positive]))\n",
    "neutral_exclam = np.sum(np.array([data[2].count('!') for data in train_neutral]))\n",
    "negative_exclam = np.sum(np.array([data[2].count('!') for data in train_negative]))\n",
    "total_exclam = positive_exclam  + neutral_exclam + negative_exclam\n",
    "p_extra[0][1] = positive_exclam / total_exclam\n",
    "p_extra[1][1] = neutral_exclam / total_exclam\n",
    "p_extra[2][1] = negative_exclam / total_exclam\n",
    "\n",
    "# 2 - Hashtags\n",
    "positive_hashtags = np.sum(np.array([data[2].count('#') for data in train_positive]))\n",
    "neutral_hashtags = np.sum(np.array([data[2].count('#') for data in train_neutral]))\n",
    "negative_hashtags = np.sum(np.array([data[2].count('#') for data in train_negative]))\n",
    "total_hashtags = positive_hashtags + neutral_hashtags + negative_hashtags\n",
    "p_extra[0][2] = positive_hashtags / total_hashtags\n",
    "p_extra[1][2] = neutral_hashtags / total_hashtags\n",
    "p_extra[2][2] = negative_hashtags / total_hashtags\n",
    "\n",
    "# 3 - Question Marks\n",
    "positive_quest = np.sum(np.array([data[2].count('?') for data in train_positive]))\n",
    "neutral_quest = np.sum(np.array([data[2].count('?') for data in train_neutral]))\n",
    "negative_quest = np.sum(np.array([data[2].count('?') for data in train_negative]))\n",
    "total_quest = positive_quest + neutral_quest + negative_quest\n",
    "p_extra[0][3] = positive_quest / total_quest\n",
    "p_extra[1][3] = neutral_quest / total_quest\n",
    "p_extra[2][3] = negative_quest / total_quest\n",
    "\n",
    "\n",
    "# 3 - Uppercase Letter\n",
    "# positive_uppercase_frequency = np.sum(np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_punc_stopwords(data):\n",
    "    for i in range(len(data)):\n",
    "        text = data[i][2].lower()\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in punctuation]\n",
    "        changed_words = [stemmer.stem(word) for word in words if word not in stopwords_set]\n",
    "        data[i][2] = ' '.join(changed_words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_data = stem_punc_stopwords(training_data)\n",
    "new_train_positive = new_training_data[new_training_data[:, 1] == 'Positive']\n",
    "new_train_neutral = new_training_data[new_training_data[:, 1] == 'Neutral']\n",
    "new_train_negative = new_training_data[new_training_data[:, 1] == 'Negative']\n",
    "text_length_positive = np.sum(np.array([len(data[2]) for data in new_train_positive]))\n",
    "text_length_neutral = np.sum(np.array([len(data[2]) for data in new_train_neutral]))\n",
    "text_length_negative = np.sum(np.array([len(data[2]) for data in new_train_negative]))\n",
    "total_text_length = text_length_positive + text_length_negative + text_length_neutral\n",
    "p_extra[0][0] = text_length_positive / total_text_length\n",
    "p_extra[1][0] = text_length_neutral / total_text_length\n",
    "p_extra[2][0] = text_length_negative / total_text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(inputData):\n",
    "    allWords = []\n",
    "    dictWord = {}\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if (word != ' ') and (word not in dictWord):\n",
    "                dictWord[word] = len(allWords)\n",
    "                allWords.append(word)\n",
    "    return (allWords, dictWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stem_punc_words, stem_punc_dict) = get_word_frequency(new_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(inputData, dictWord):\n",
    "    p_wc = np.zeros((3, len(dictWord)))\n",
    "    for data in inputData:\n",
    "        text = word_tokenize(data[2])\n",
    "        for word in text:\n",
    "            if word not in dictWord: continue\n",
    "            if(data[1] == 'Positive'): p_wc[0][dictWord[word]] += 1\n",
    "            elif(data[1] == 'Neutral'): p_wc[1][dictWord[word]] += 1\n",
    "            else: p_wc[2][dictWord[word]] += 1\n",
    "    total = [0, 0, 0]\n",
    "    total[0] = sum(p_wc[0])\n",
    "    total[1] = sum(p_wc[1])\n",
    "    total[2] = sum(p_wc[2])\n",
    "    for i in range(len(total)):\n",
    "        for j in range(len(p_wc[i])):\n",
    "            p_wc[i][j] = (p_wc[i][j] + 1) / (total[i] + len(dictWord))\n",
    "    return p_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wc_stem_punc = parameters(new_training_data, stem_punc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_punc_predict(text, dictWord, pc, p_wc, p_extra):\n",
    "    p = np.zeros((3,))\n",
    "    # 0 - Length, 1 - !, 2 - #, 3 - ?\n",
    "    text_exclam = text.count('!')\n",
    "    text_quest = text.count('?')\n",
    "    text_hash = text.count('#')\n",
    "    lowered_text = text.lower()\n",
    "    lowered_text = word_tokenize(lowered_text)\n",
    "    upper_text = word_tokenize(text)\n",
    "    uppercase_words = []\n",
    "    words = []\n",
    "    for i in range(len(lowered_text)):\n",
    "        if(lowered_text[i] in punctuation): continue\n",
    "        uppercase_words.append(upper_text[i])\n",
    "        words.append(lowered_text[i])\n",
    "    for i in range(len(p)):\n",
    "        p[i] = np.log(pc[i])\n",
    "        p[i] += text_exclam * p_extra[i][1]\n",
    "        p[i] += text_quest * p_extra[i][3]\n",
    "        p[i] += text_hash * p_extra[i][2]\n",
    "        # p[i] += text_len * p_extra[i][0]\n",
    "        for j in range(len(words)):\n",
    "            if(words[j] in stopwords_set): continue\n",
    "            capitalised = 0\n",
    "            for char in uppercase_words[j]:\n",
    "                if(char >= 'A' and char <= 'Z'): capitalised += 1\n",
    "            total = len(uppercase_words[j])\n",
    "            multiplier = (1 + capitalised/total)\n",
    "            words[j] = stemmer.stem(words[j])\n",
    "            if(words[j] in dictWord):\n",
    "                p[i] += multiplier * np.log(p_wc[i][dictWord[words[j]]])\n",
    "    return np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_punc_train_correct = 0\n",
    "stem_punc_train_incorrect = 0\n",
    "for data in training_data:\n",
    "    prediction = stem_punc_predict(data[2], stem_punc_dict, pc, p_wc_stem_punc, p_extra)\n",
    "    if(data[1] == 'Positive' and prediction == 0): stem_punc_train_correct += 1\n",
    "    elif(data[1] == 'Neutral' and prediction == 1): stem_punc_train_correct += 1\n",
    "    elif(data[1] == 'Negative' and prediction == 2): stem_punc_train_correct += 1\n",
    "    else: stem_punc_train_incorrect += 1\n",
    "stem_punc_val_correct = 0\n",
    "stem_punc_val_incorrect = 0\n",
    "for data in validation_data:\n",
    "    prediction = stem_punc_predict(data[2], stem_punc_dict, pc, p_wc_stem_punc, p_extra)\n",
    "    if(data[1] == 'Positive' and prediction == 0): stem_punc_val_correct += 1\n",
    "    elif(data[1] == 'Neutral' and prediction == 1): stem_punc_val_correct += 1\n",
    "    elif(data[1] == 'Negative' and prediction == 2): stem_punc_val_correct += 1\n",
    "    else: stem_punc_val_incorrect += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"New Features, Training\\nCorrect: {stem_punc_train_correct}\\nIncorrect: {stem_punc_train_incorrect}\\nAccuracy: {stem_punc_train_correct / (stem_punc_train_correct + stem_punc_train_incorrect)}\")\n",
    "print(f\"New Features, Validation\\nCorrect: {stem_punc_val_correct}\\nIncorrect: {stem_punc_val_incorrect}\\nAccuracy: {stem_punc_val_correct / (stem_punc_val_correct + stem_punc_val_incorrect)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_extra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
